{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!python3 -m pip install ../input/nflimpactdetectron/detectron2/detectron2 --no-index --find-links ../input/nflimpactdetectron/wheelz/wheelz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfrom detectron2.config import get_cfg\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.engine import DefaultPredictor,DefaultTrainer\nfrom detectron2.model_zoo import model_zoo\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader, build_detection_test_loader, \\\n    DatasetMapper\nfrom detectron2.structures import BoxMode, Instances, Boxes\nfrom detectron2.modeling import build_model\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.structures import Instances\n\nfrom detectron2.modeling.roi_heads.roi_heads import StandardROIHeads\nfrom detectron2.modeling import ROI_HEADS_REGISTRY\n\nfrom typing import Dict, List, Optional, Tuple\n\nfrom detectron2.utils.visualizer import Visualizer\n\nimport cv2\nimport torch\nfrom glob import glob\nfrom copy import deepcopy \nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/footballhelmetdetector')\nfrom model import get_predictor\n# \\, new_model_cfg\nfrom train_helmet_detector import NflImpactTrainer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls extracted_test_video","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n\nROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nOUTPUT = ROOT / \"working\"\nDATA = INPUT / \"nfl-impact-detection\"\nTRAIN_VIDEOS = DATA / \"train\"\nTEST_VIDEOS = DATA / \"test\"\n\n# \nEXTRACTED_TEST_VIDEOS = OUTPUT / \"extracted_test_video\"\n\nWORK = ROOT / \"working\"\n\n# # save frames out of /kaggle/working/ because of the HDD limitation\nTMP = ROOT / \"tmp\"\nTRAIN_EXTRACTED_FRAMES = TMP / \"nfl-impact-detection-train-frames\"\n\ntry:\n    TRAIN_EXTRACTED_FRAMES.mkdir(parents=True)\nexcept:\n    print('file exist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact == 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}//{video_name}'.replace('.mp4',f'_{str(frame).zfill(3)}.png')\n        _ = cv2.imwrite(image_path, img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !mkdir test\n# !cp ../input/nfl-impact-detection/test/57906_000718_Endzone.mp4 ./test/57906_000718_Endzone.mp4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extracted image from test video into DATA_ROOT_PATH"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n!rm -r helmet_crop_test\nout_dir = EXTRACTED_TEST_VIDEOS\n\nif not os.path.exists(out_dir):\n    !mkdir -p $out_dir\n    video_dir = '/kaggle/input/nfl-impact-detection/test'\n    uniq_video = [path.split('/')[-1] for path in glob(f'{video_dir}/*.mp4')]\n    for video_name in uniq_video:\n        mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)\nelse:\n    print('can not extracted video to frame')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_model_cfg():\n    cfg = get_cfg()\n    model = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n    cfg.merge_from_file(model_zoo.get_config_file(model))\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model)\n    cfg.MODEL.MASK_ON = False\n    cfg.INPUT.RANDOM_FLIP = \"none\"\n    cfg.OUTPUT_DIR = \"output\"\n\n    cfg.DATALOADER.NUM_WORKERS = 2\n    cfg.SOLVER.IMS_PER_BATCH = 4\n    cfg.SOLVER.CHECKPOINT_PERIOD = 2000\n    cfg.SOLVER.STEPS = (21000, 50000)\n    cfg.SOLVER.MAX_ITER = 200000\n    cfg.SOLVER.BASE_LR = 0.001\n    cfg.TEST.EVAL_PERIOD = 2000\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n    cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True\n#     cfg.MODEL.DEVICE = 'cpu'\n    return cfg\n\n\ndef get_predictor(path = \"../input/nflimpactdetectron/helmet_detector_model_0093999.pth\"):\n    classes = ['helmet']\n    MetadataCatalog.get(\"nflimpact\").set(thing_classes=classes)\n    cfg = new_model_cfg()\n    cfg.MODEL.WEIGHTS = path\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n    return DefaultPredictor(cfg)\n\n\n# predictor = build_model(cfg)\n\n# images = ImageList.from_tensors()  # preprocessed input tensor\n\n# model = build_model(cfg)\n# model.eval()\n# features = model.backbone(images.tensor)\n# proposals, _ = model.proposal_generator(images, features)\n# instances, _ = model.roi_heads(images, features, proposals)\n# mask_features = [features[f] for f in model.roi_heads.in_features]\n# mask_features = model.roi_heads.mask_pooler(mask_features, [x.pred_boxes for x in instances])\n\n# predictor = get_predictor('../input/run-detectron-helmet-detector/output/model_0001999.pth') # e2e\n\n\npredictor = get_predictor('../input/nflimpactdetectron/helmet_detector_model_0093999.pth') # box predictor only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bboxes2xywh(df):\n    # row to multiple rows     \n    df = df[df.bboxes.apply(lambda x: len(x)!=0)]\n    df = df.explode('bboxes').reset_index(drop=True)\n    \n#     df['bboxes'] = df['bboxes'].apply(lambda x: x.tensor.to('cpu').numpy()[0])\n    \n    df[['x','y','w','h']] = pd.DataFrame(df.bboxes.tolist(), index= df.index)\n\n    df['w'] = df['w'] - df['x']\n    df['h'] = df['h'] - df['y']\n    df.x = df.x.round()\n    df.y = df.y.round()\n    df.w = df.w.round()\n    df.h = df.h.round()\n    \n    df = df.rename(columns={\"x\": \"left\", \"w\":\"width\", \"y\": \"top\", \"h\":\"height\"})\n    df = df.drop(columns=['bboxes'])\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get bboxes of helmet using 2nd stage detectron2"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# path = '../input/nfl-impact-detection-train-frames/57583_000082_Endzone'\ndic_list = []\n\nfor path in os.listdir(EXTRACTED_TEST_VIDEOS):\n    gameKey, playID, view, frame = path.split('_')\n    video = '_'.join([gameKey, playID, view]) + '.mp4'\n    gameKey = int(gameKey)\n    playID = int(playID)\n    frame = int(frame[:-4])\n    im = cv2.imread(f\"{EXTRACTED_TEST_VIDEOS}/{path}\")\n#     print(im)\n\n    model_output = predictor(im)\n    \n    model_output = model_output[\"instances\"].to(\"cpu\")\n    filter_mask = model_output.scores > 0.8\n    ni = Instances(model_output.image_size, **{\n        \"scores\": model_output.scores[filter_mask],\n        \"pred_boxes\": model_output.pred_boxes[filter_mask],\n        \"pred_classes\": model_output.pred_classes[filter_mask]\n    })\n    \n#     crop bigger bboxes from output of object detection\n\n    bboxes = ni.get('pred_boxes').tensor.numpy()\n\n    dic = {'gameKey':gameKey, 'playID':playID, 'view':view, 'video':video, 'frame':frame, 'bboxes':bboxes,\n#           'video':video\n          }\n    dic_list.append(dic)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('dic_list.pickle', 'wb') as handle:\n    pickle.dump(dic_list, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(dic_list)\ndf = bboxes2xywh(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Careful convert btw xywh and xyxy here"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Call this for the output of detectron\n\ndef mk_crop_imgs(df,split='train', input_folder = \"../input/nfl-impact-detection-train-frames/\", is_nested_folder = True):\n    max_w, max_h = 1280, 720\n    def _convert_tl_br(magnitude, x1,y1,w_box,h_box):\n        assert magnitude >0, 'increase size <0'\n        x2 = x1 + w_box\n        y2 = y1 + h_box\n        w_box = abs(x2 - x1)*magnitude\n        assert w_box >0, 'w_box <0'\n        h_box = abs(y2 - y1)*magnitude\n        assert h_box >0, 'h_box <0'\n\n        x1 = x1 - w_box if (x1 - w_box) >0 else 0\n        y1 = y1 - h_box if (y1 - h_box) >0 else 0\n\n        x2 = x2 + w_box if (x2 + w_box) <max_w else max_w\n        y2 = y2 + h_box if (y2 + h_box) <max_h else max_h\n\n        return int(x1),int(y1),int(x2),int(y2)\n    \n    def _convert_tl_br_fixed_size(size, x,y,w,h):\n        h_target,w_target = size\n        # if original box is smaller than target box   \n        if(w<= w_target and h<= h_target):\n            center_x = x+ np.floor(w/2)\n            center_y = y+ np.floor(h/2)\n\n            x1 = center_x - np.floor(w_target/2) if center_x - np.floor(w_target/2) >0 else 0\n            y1 = center_y - np.floor(h_target/2) if center_y - np.floor(h_target/2) >0 else 0\n\n            x2 = center_x + np.floor(w_target/2) if center_x + np.floor(w_target/2) < max_w else max_w\n            y2 = center_y + np.floor(h_target/2) if center_y + np.floor(h_target/2) < max_h else max_h\n        # if original box is bigger than target box   \n        else:\n            x1 = x\n            y1 = y\n            x2 = x + w\n            y2 = y + h\n        return int(x1),int(y1),int(x2),int(y2)\n\n\n    def crop_img(row):\n        # setup path\n        \n        input_name = row['image_name']\n        \n        if (not('label' in row.keys()) and not('impact' in row.keys()) ):\n#             Need to change this to distinguish btw small bboxes, use index of df ???\n            output_name = input_name\n        else:\n            playerLabel = row['label']\n            impact = int(row['impact'])\n            output_name = f\"{input_name[:-4]}_{playerLabel}_{impact}.png\"\n        \n        \n        path = f\"helmet_crop_{split}/\"\n        \n        \n        #         convert coordinate to be magnitude**2 time bigger\n        #x1,y1,x2,y2 = _convert_tl_br(1, *row[['x','y','w','h']])\n        # convert bboxes to fix size\n        x1,y1,x2,y2 =_convert_tl_br_fixed_size((100,100), *row[['x','y','w','h']])\n        \n        # final path to input image       \n        if is_nested_folder:\n            nested_folder = row['video'].replace('.mp4', '')\n            big_img_path = os.path.join(input_folder,nested_folder, row['image_name'])\n        else:\n            big_img_path = os.path.join(input_folder, row['image_name'])\n#         print(big_img_path)\n        img = cv2.imread(big_img_path)\n\n        assert img.shape.__len__() == 3 , 'image is incorrect'\n        crop_img = img[y1:y2, x1:x2]\n        \n        \n        \n        w_box = abs(x2 - x1)\n        h_box = abs(y2 - y1)\n        if(w_box < 100 or h_box<100):\n            try:\n                resized_image = cv2.resize(crop_img, (100, 100)) \n                cv2.imwrite(os.path.join(path,output_name), resized_image)\n    #             print('write img ok')\n            except:\n                print(x1,y1,x2,y2)\n                print(f\"crop_img = {crop_img.shape}\")\n                print(f\"resize_img = {resized_image.shape}.\")\n                print(f\"smt wrong with {output_name} at w_box < 100 or h_box<100\")\n        else:\n            try:\n                \n                cv2.imwrite(os.path.join(path, output_name), crop_img)\n    #             print('write img ok')\n            except:\n                print(x1,y1,x2,y2)\n                print(f\"crop_img = {crop_img.shape}\")\n                print(f\"smt wrong with {output_name}\")\n    df = df.reset_index(drop=True)\n    \n    for index,row in df.iterrows():\n        crop_img(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_name_df(df):\n    def convert_name(input_row):\n    #     print(row)\n        row = input_row.copy()\n        input_name = row.image_name\n        if (not('label' in row.keys()) and not('impact' in row.keys()) ):\n            return input_name\n        else:\n            return f\"{input_name[:-4]}_{playerLabel}_{int(impact)}.png\" \n    df['image_name'] = df.apply(lambda x: x['video'].replace('.mp4', '') + '_' + str(x['frame']).zfill(3) + '.png', axis=1)\n    df['image_name'] = df.apply(lambda x: convert_name(x), axis=1)\n#     df['label'] = '666'\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Crop image to 100x100 size from video frame (image) "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = convert_name_df(df)\ndf = df.rename(columns={\"left\": \"x\", \"width\":\"w\", \"top\": \"y\", \"height\":\"h\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmk_crop_imgs(df, input_folder= EXTRACTED_TEST_VIDEOS, split='test', is_nested_folder = False) #this will output to ./helmet_crop_test\n# resnet on this df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageDatasetBoxes(Dataset) :\n    def __init__(self, df=None, transform=None, img_size=(100,100), mask= False, input_folder=\"../input/resnet18-on-bboxes/\", name=\"train\") :\n        self.df = df\n        self.img_size = img_size\n        self.transform = transform\n        self.mask = mask\n        \n#         print(f\"name={name}\")\n        \n        if name == 'train':\n            self.input_folder = os.path.join(input_folder,'helmet_crop_train')\n        elif name == 'test':\n            self.input_folder = os.path.join(input_folder,'helmet_crop_test')\n            \n    def __len__(self) :\n        return len(self.df)\n    \n    # fix this for inference\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.input_folder, row.image_name)\n        image = cv2.imread(img_path)\n        if self.transform:\n            transformed_img = train_transform(image=image)['image']\n\n        if name == \"train\":\n            if int(row.impact) == 0:\n                label = torch.tensor([1,0])\n            elif int(row.impact) == 1:\n                label = torch.tensor([0,1])\n            else:\n                print(row.impact)\n                print(type(row.impact))\n                assert False, 'incorrect label type'\n            return transformed_img, label\n        \n        elif name == \"test\":\n            return transformed_img, row\n        else:\n            assert name !== 'test' and name !== 'train', ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = A.Compose(\n    [\n        Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n        ),\n        ToTensor()\n    ]\n)\n\n# TODO: include input folder \ntest_dataset = ImageDatasetBoxes(submit_df,transform= transform, input_folder= EXTRACTED_TEST_VIDEOS,= name = \"test\")\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torch.load('resnet_epoch_no_mask4.pt')['model']\npredicts = []\noutput_rows = []\n\nmodel.eval()\nwith torch.no_grad():            \n#     with torch.set_grad_enabled(False):\n    for images, rows in tqdm(test_loader) :\n        images = images.to(device)             \n        l = l.to(device)\n\n        out = model(image)\n        out_ = out.argmax(1)\n        out_ = out_.to('cpu').numpy().tolist()\n        \n        df_rows = pd.DataFrame(rows)\n        df_rows['impact'] = out_\n        \n        output_rows.append(df_rows)\n\nfinal_submit_df = pd.DataFrame(output_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mk_crop_imgs(df,split='train'):\n    max_w, max_h = 1280, 720\n    def _convert_tl_br(magnitude, x1,y1,w_box,h_box):\n        assert magnitude >0, 'increase size <0'\n        x2 = x1 + w_box\n        y2 = y1 + h_box\n        w_box = abs(x2 - x1)*magnitude\n        assert w_box >0, 'w_box <0'\n        h_box = abs(y2 - y1)*magnitude\n        assert h_box >0, 'h_box <0'\n\n        x1 = x1 - w_box if (x1 - w_box) >0 else 0\n        y1 = y1 - h_box if (y1 - h_box) >0 else 0\n\n        x2 = x2 + w_box if (x2 + w_box) <max_w else max_w\n        y2 = y2 + h_box if (y2 + h_box) <max_h else max_h\n\n        return int(x1),int(y1),int(x2),int(y2)\n    \n    def _convert_tl_br_fixed_size(size, x,y,w,h):\n        h_target,w_target = size\n        # if original box is smaller than target box   \n        if(w<= w_target and h<= h_target):\n            center_x = x+ np.floor(w/2)\n            center_y = y+ np.floor(h/2)\n\n            x1 = center_x - np.floor(w_target/2) if center_x - np.floor(w_target/2) >0 else 0\n            y1 = center_y - np.floor(h_target/2) if center_y - np.floor(h_target/2) >0 else 0\n\n            x2 = center_x + np.floor(w_target/2) if center_x + np.floor(w_target/2) < max_w else max_w\n            y2 = center_y + np.floor(h_target/2) if center_y + np.floor(h_target/2) < max_h else max_h\n        # if original box is bigger than target box   \n        else:\n            x1 = x\n            y1 = y\n            x2 = x + w\n            y2 = y + h\n        return int(x1),int(y1),int(x2),int(y2)\n\n\n    def crop_img(row):\n        #         setup path\n        input_folder = \"../input/nfl-impact-detection-train-frames/\"\n        input_name = row['image_name']\n        playerLabel = row['label']\n        path = f\"helmet_crop_{split}/\"\n        \n        #         convert coordinate to be magnitude**2 time bigger\n        #x1,y1,x2,y2 = _convert_tl_br(1, *row[['x','y','w','h']])\n        # convert bboxes to fix size\n        x1,y1,x2,y2 =_convert_tl_br_fixed_size((100,100), *row[['x','y','w','h']])\n        \n        input_path = row['video'].replace('.mp4', '')\n        \n        # final path to input image         \n        the_path = os.path.join(input_folder,input_path, row['image_name'])\n#         print(f\"{the_path}\")\n        img = cv2.imread(the_path)\n        \n        assert img.shape.__len__() == 3, 'incorrect image'\n        crop_img = img[y1:y2, x1:x2]\n        \n        output_name = f\"{input_name[:-4]}_{playerLabel}_{int(row['impact'])}.png\"\n        \n        w_box = abs(x2 - x1)\n        h_box = abs(y2 - y1)\n        if(w_box < 100 or h_box<100):\n            try:\n                resized_image = cv2.resize(crop_img, (100, 100)) \n                cv2.imwrite(os.path.join(path,output_name), resized_image)\n    #             print('write img ok')\n            except:\n                print(x1,y1,x2,y2)\n                print(f\"crop_img = {crop_img.shape}\")\n                print(f\"resize_img = {resized_image.shape}.\")\n                print(f\"smt wrong with {output_name} at w_box < 100 or h_box<100\")\n        else:\n            try:\n                \n                cv2.imwrite(os.path.join(path,output_name), crop_img)\n    #             print('write img ok')\n            except:\n                print(x1,y1,x2,y2)\n                print(f\"crop_img = {crop_img.shape}\")\n                print(f\"smt wrong with {output_name}\")\n    df = df.reset_index(drop=True)\n    for index,row in tqdm(df.iterrows()):\n#         print(index)\n        crop_img(row)\n#         break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nflimpact\nenv = nflimpact.make_env()\nenv.predict(final_submit_df) # df is a pandas dataframe of your entire submission file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/utils-to-prepare-different-type-of-dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}